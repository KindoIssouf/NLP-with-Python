{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikindo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\ikindo\\AppData\\Local\\Temp/ipykernel_16828/3500698544.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Vote_42k_tweets.csv', error_bad_lines=False);\n",
    "data_text = data[['Tweet']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@TesCalendar Also, my partner was complaining ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imagine elon tweet tommorow tesla accepting bi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Filasophical: IMHO, @Tesla &amp;amp; @elonmusk...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @elonmusk: With https://t.co/45B5nUBaxb &amp;am...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @EvaFoxU: Tesla CEO Elon Musk &amp;amp; AMD CEO...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  index\n",
       "0  @TesCalendar Also, my partner was complaining ...      0\n",
       "1  Imagine elon tweet tommorow tesla accepting bi...      1\n",
       "2  RT @Filasophical: IMHO, @Tesla &amp; @elonmusk...      2\n",
       "3  RT @elonmusk: With https://t.co/45B5nUBaxb &am...      3\n",
       "4  RT @EvaFoxU: Tesla CEO Elon Musk &amp; AMD CEO...      4"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ikindo\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ikindo\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ikindo\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ikindo\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\ikindo\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ikindo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Tesla', 'base', 'is', 'growing', 'exponentially', '\\n\\nTesla', 'profits', 'are', 'growing', 'exponentially', '\\n\\n$TSLA', 'Twitter', 'Is', 'growing', 'exponentia…', 'https://t.co/w7OpNUunzh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['tesla', 'base', 'grow', 'exponenti', 'tesla', 'profit', 'grow', 'exponenti', 'tsla', 'twitter', 'grow', 'exponentia', 'https', 'opnuunzh']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [tescalendar, partner, complain, tesla, spare,...\n",
       "1    [imagin, elon, tweet, tommorow, tesla, accept,...\n",
       "2    [filasoph, imho, tesla, elonmusk, lead, revolu...\n",
       "3    [elonmusk, https, nubaxb, powerwal, batteri, u...\n",
       "4    [evafoxu, tesla, elon, musk, lisa, thank, good...\n",
       "5    [funim, order, save, world, destruct, spi, tea...\n",
       "6                [shahfromthec, manifest, like, tesla]\n",
       "7    [tesla, solar, roof, generat, clean, energi, a...\n",
       "8    [funim, order, save, world, destruct, spi, tea...\n",
       "9    [funim, order, save, world, destruct, spi, tea...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "processed_docs = documents['Tweet'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Tesla', 'base', 'is', 'growing', 'exponentially', '\\n\\nTesla', 'profits', 'are', 'growing', 'exponentially', '\\n\\n$TSLA', 'Twitter', 'Is', 'growing', 'exponentia…', 'https://t.co/w7OpNUunzh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['tesla', 'base', 'grow', 'exponenti', 'tesla', 'profit', 'grow', 'exponenti', 'tsla', 'twitter', 'grow', 'exponentia', 'https', 'opnuunzh']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['Tweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [tescalendar, partner, complain, tesla, spare,...\n",
       "1    [imagin, elon, tweet, tommorow, tesla, accept,...\n",
       "2    [filasoph, imho, tesla, elonmusk, lead, revolu...\n",
       "3    [elonmusk, https, nubaxb, powerwal, batteri, u...\n",
       "4    [evafoxu, tesla, elon, musk, lisa, thank, good...\n",
       "5    [funim, order, save, world, destruct, spi, tea...\n",
       "6                [shahfromthec, manifest, like, tesla]\n",
       "7    [tesla, solar, roof, generat, clean, energi, a...\n",
       "8    [funim, order, save, world, destruct, spi, tea...\n",
       "9    [funim, order, save, world, destruct, spi, tea...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 complain\n",
      "1 go\n",
      "2 https\n",
      "3 look\n",
      "4 ncqemsfhiv\n",
      "5 partner\n",
      "6 spare\n",
      "7 tescalendar\n",
      "8 tesla\n",
      "9 trunk\n",
      "10 accept\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (278, 1), (344, 1), (456, 1), (522, 1), (914, 2), (1479, 3)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"https\") appears 1 time.\n",
      "Word 278 (\"twitter\") appears 1 time.\n",
      "Word 344 (\"tsla\") appears 1 time.\n",
      "Word 456 (\"profit\") appears 1 time.\n",
      "Word 522 (\"base\") appears 1 time.\n",
      "Word 914 (\"exponenti\") appears 2 time.\n",
      "Word 1479 (\"grow\") appears 3 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.6217392477195879),\n",
      " (1, 0.31936274828689604),\n",
      " (2, 0.0797131177971159),\n",
      " (3, 0.3561900409971062),\n",
      " (4, 0.6149977369041815)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.083*\"https\" + 0.061*\"elonmusk\" + 0.017*\"amaz\" + 0.016*\"sawyermerritt\" + 0.013*\"iphon\" + 0.013*\"tsla\" + 0.012*\"make\" + 0.011*\"compani\" + 0.011*\"laker\" + 0.011*\"drive\"\n",
      "Topic: 1 \n",
      "Words: 0.051*\"enter\" + 0.044*\"model\" + 0.042*\"retweet\" + 0.040*\"want\" + 0.039*\"https\" + 0.037*\"giveaway\" + 0.034*\"shib\" + 0.030*\"elonmusk\" + 0.028*\"accept\" + 0.027*\"love\"\n",
      "Topic: 2 \n",
      "Words: 0.055*\"elonmusk\" + 0.045*\"poll\" + 0.040*\"https\" + 0.038*\"doge\" + 0.035*\"dogecoin\" + 0.034*\"accept\" + 0.030*\"musk\" + 0.026*\"zerohedg\" + 0.019*\"elon\" + 0.017*\"power\"\n",
      "Topic: 3 \n",
      "Words: 0.040*\"https\" + 0.037*\"elonmusk\" + 0.025*\"person\" + 0.022*\"bitcoin\" + 0.021*\"year\" + 0.020*\"achiev\" + 0.019*\"money\" + 0.017*\"appl\" + 0.016*\"thread\" + 0.016*\"tsla\"\n",
      "Topic: 4 \n",
      "Words: 0.076*\"sell\" + 0.053*\"billion\" + 0.051*\"stock\" + 0.040*\"elon\" + 0.039*\"musk\" + 0.027*\"worth\" + 0.026*\"week\" + 0.026*\"gain\" + 0.022*\"unreal\" + 0.019*\"avoid\"\n",
      "Topic: 5 \n",
      "Words: 0.078*\"https\" + 0.020*\"need\" + 0.020*\"think\" + 0.015*\"know\" + 0.014*\"come\" + 0.013*\"like\" + 0.013*\"elonmusk\" + 0.012*\"hope\" + 0.012*\"flow\" + 0.012*\"work\"\n",
      "Topic: 6 \n",
      "Words: 0.095*\"elon\" + 0.089*\"musk\" + 0.085*\"https\" + 0.030*\"solv\" + 0.018*\"offici\" + 0.017*\"say\" + 0.013*\"texa\" + 0.012*\"announc\" + 0.012*\"break\" + 0.011*\"form\"\n",
      "Topic: 7 \n",
      "Words: 0.076*\"sell\" + 0.063*\"https\" + 0.058*\"stock\" + 0.037*\"twitter\" + 0.030*\"elonmusk\" + 0.027*\"tsla\" + 0.020*\"share\" + 0.017*\"world\" + 0.016*\"musk\" + 0.013*\"sept\"\n",
      "Topic: 8 \n",
      "Words: 0.090*\"https\" + 0.032*\"elonmusk\" + 0.031*\"propos\" + 0.029*\"model\" + 0.023*\"product\" + 0.017*\"hertz\" + 0.015*\"true\" + 0.014*\"test\" + 0.014*\"samtwit\" + 0.014*\"cash\"\n",
      "Topic: 9 \n",
      "Words: 0.070*\"doge\" + 0.053*\"elonmusk\" + 0.043*\"https\" + 0.033*\"away\" + 0.033*\"give\" + 0.030*\"babi\" + 0.027*\"model\" + 0.025*\"winner\" + 0.021*\"giveaway\" + 0.019*\"enter\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.022*\"laker\" + 0.020*\"wild\" + 0.020*\"https\" + 0.016*\"elonmusk\" + 0.014*\"kunqvb\" + 0.010*\"model\" + 0.009*\"tsla\" + 0.007*\"wholemarsblog\" + 0.006*\"rang\" + 0.006*\"deliveri\"\n",
      "Topic: 1 Word: 0.016*\"prize\" + 0.013*\"https\" + 0.012*\"tsla\" + 0.009*\"year\" + 0.009*\"bitcoin\" + 0.009*\"underestim\" + 0.008*\"return\" + 0.008*\"elonmusk\" + 0.007*\"nvidia\" + 0.007*\"model\"\n",
      "Topic: 2 Word: 0.027*\"https\" + 0.018*\"amaz\" + 0.017*\"miss\" + 0.016*\"guy\" + 0.016*\"chanc\" + 0.014*\"model\" + 0.014*\"shibaarch\" + 0.013*\"iphon\" + 0.010*\"elonmusk\" + 0.008*\"tomorrow\"\n",
      "Topic: 3 Word: 0.045*\"accept\" + 0.045*\"want\" + 0.044*\"shib\" + 0.036*\"retweet\" + 0.027*\"shibainuhodl\" + 0.017*\"doge\" + 0.017*\"hertz\" + 0.017*\"shibaa_token\" + 0.015*\"babi\" + 0.011*\"elonmusk\"\n",
      "Topic: 4 Word: 0.034*\"musk\" + 0.029*\"elon\" + 0.028*\"sell\" + 0.022*\"stock\" + 0.021*\"poll\" + 0.017*\"billion\" + 0.017*\"twitter\" + 0.015*\"https\" + 0.014*\"say\" + 0.012*\"decid\"\n",
      "Topic: 5 Word: 0.017*\"stock\" + 0.016*\"gain\" + 0.014*\"sell\" + 0.014*\"avoid\" + 0.013*\"late\" + 0.013*\"mean\" + 0.012*\"propos\" + 0.012*\"twitter\" + 0.012*\"elonmusk\" + 0.011*\"unreal\"\n",
      "Topic: 6 Word: 0.023*\"hunger\" + 0.021*\"world\" + 0.020*\"solv\" + 0.018*\"right\" + 0.017*\"elonmusk\" + 0.015*\"exact\" + 0.015*\"thread\" + 0.013*\"like\" + 0.013*\"drelidavid\" + 0.013*\"https\"\n",
      "Topic: 7 Word: 0.013*\"https\" + 0.011*\"enter\" + 0.010*\"elonmusk\" + 0.009*\"mint\" + 0.009*\"give\" + 0.009*\"away\" + 0.009*\"sawyermerritt\" + 0.008*\"model\" + 0.008*\"wholemarsblog\" + 0.008*\"nftgiveaway\"\n",
      "Topic: 8 Word: 0.042*\"doge\" + 0.020*\"elonmusk\" + 0.017*\"investments_ceo\" + 0.014*\"accept\" + 0.013*\"dogecoin\" + 0.011*\"https\" + 0.009*\"high\" + 0.009*\"power\" + 0.008*\"elonmu\" + 0.008*\"year\"\n",
      "Topic: 9 Word: 0.021*\"elonmusk\" + 0.014*\"enter\" + 0.014*\"https\" + 0.012*\"love\" + 0.010*\"cryptobri_\" + 0.009*\"giveaway\" + 0.009*\"lmao\" + 0.008*\"custom\" + 0.008*\"collect\" + 0.008*\"babydogecoin\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tesla',\n",
       " 'base',\n",
       " 'grow',\n",
       " 'exponenti',\n",
       " 'tesla',\n",
       " 'profit',\n",
       " 'grow',\n",
       " 'exponenti',\n",
       " 'tsla',\n",
       " 'twitter',\n",
       " 'grow',\n",
       " 'exponentia',\n",
       " 'https',\n",
       " 'opnuunzh']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5025003552436829\t \n",
      "Topic: 0.040*\"https\" + 0.037*\"elonmusk\" + 0.025*\"person\" + 0.022*\"bitcoin\" + 0.021*\"year\" + 0.020*\"achiev\" + 0.019*\"money\" + 0.017*\"appl\" + 0.016*\"thread\" + 0.016*\"tsla\"\n",
      "\n",
      "Score: 0.32302671670913696\t \n",
      "Topic: 0.078*\"https\" + 0.020*\"need\" + 0.020*\"think\" + 0.015*\"know\" + 0.014*\"come\" + 0.013*\"like\" + 0.013*\"elonmusk\" + 0.012*\"hope\" + 0.012*\"flow\" + 0.012*\"work\"\n",
      "\n",
      "Score: 0.1108095571398735\t \n",
      "Topic: 0.076*\"sell\" + 0.053*\"billion\" + 0.051*\"stock\" + 0.040*\"elon\" + 0.039*\"musk\" + 0.027*\"worth\" + 0.026*\"week\" + 0.026*\"gain\" + 0.022*\"unreal\" + 0.019*\"avoid\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6261741518974304\t \n",
      "Topic: 0.017*\"stock\" + 0.016*\"gain\" + 0.014*\"sell\" + 0.014*\"avoid\" + 0.013*\"late\" + 0.013*\"mean\" + 0.012*\"propos\" + 0.012*\"twitter\" + 0.012*\"elonmusk\" + 0.011*\"unreal\"\n",
      "\n",
      "Score: 0.3010769188404083\t \n",
      "Topic: 0.013*\"https\" + 0.011*\"enter\" + 0.010*\"elonmusk\" + 0.009*\"mint\" + 0.009*\"give\" + 0.009*\"away\" + 0.009*\"sawyermerritt\" + 0.008*\"model\" + 0.008*\"wholemarsblog\" + 0.008*\"nftgiveaway\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5499617457389832\t Topic: 0.083*\"https\" + 0.061*\"elonmusk\" + 0.017*\"amaz\" + 0.016*\"sawyermerritt\" + 0.013*\"iphon\"\n",
      "Score: 0.05001309886574745\t Topic: 0.070*\"doge\" + 0.053*\"elonmusk\" + 0.043*\"https\" + 0.033*\"away\" + 0.033*\"give\"\n",
      "Score: 0.05000410974025726\t Topic: 0.095*\"elon\" + 0.089*\"musk\" + 0.085*\"https\" + 0.030*\"solv\" + 0.018*\"offici\"\n",
      "Score: 0.050003789365291595\t Topic: 0.055*\"elonmusk\" + 0.045*\"poll\" + 0.040*\"https\" + 0.038*\"doge\" + 0.035*\"dogecoin\"\n",
      "Score: 0.05000344291329384\t Topic: 0.051*\"enter\" + 0.044*\"model\" + 0.042*\"retweet\" + 0.040*\"want\" + 0.039*\"https\"\n",
      "Score: 0.05000291392207146\t Topic: 0.076*\"sell\" + 0.063*\"https\" + 0.058*\"stock\" + 0.037*\"twitter\" + 0.030*\"elonmusk\"\n",
      "Score: 0.050002727657556534\t Topic: 0.090*\"https\" + 0.032*\"elonmusk\" + 0.031*\"propos\" + 0.029*\"model\" + 0.023*\"product\"\n",
      "Score: 0.050002723932266235\t Topic: 0.040*\"https\" + 0.037*\"elonmusk\" + 0.025*\"person\" + 0.022*\"bitcoin\" + 0.021*\"year\"\n",
      "Score: 0.050002723932266235\t Topic: 0.076*\"sell\" + 0.053*\"billion\" + 0.051*\"stock\" + 0.040*\"elon\" + 0.039*\"musk\"\n",
      "Score: 0.050002723932266235\t Topic: 0.078*\"https\" + 0.020*\"need\" + 0.020*\"think\" + 0.015*\"know\" + 0.014*\"come\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'I hate tesla'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
